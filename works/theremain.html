<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../css/style.css">
	<link rel="icon" href="../techimg/logo_icon_2.0.gif">
	<title>David Santiano</title>
</head>
<body onload = "setRainbowElementColors()">
	<nav class="sidebar">
		<img id = "side_bar_logo" src="../techimg/rainbowdropdave_opacity_down.png" alt="logo_name">
		<div id = "nav_links">
			<a href="../about/index.html" id="about" class = "rainbow_element">/About/</a>
			<a href="../" id="projects" class = "rainbow_element">/Projects/</a>
			<!-- <a href="../sketches/index.html" id = "sketches"class = "rainbow_element">/Sketches/</a> -->
		</div>
	</nav>

	<article class="content">
		<h1>Therem{AI}n</h1>
		2018/AI/Music
		<p>
			Collaborators:<a class = "rainbow_element obvious-link" href =
			"http://samjhhu.com/">Sam Hu</a> and <span class =
			"rainbow_element">Aven Zhou</span>
		</p>
		<img src = "../techimg/there-1.gif" class ="project_image">
		<p>
			What this project was aiming to do was to create a machine learning
			+ human musician relationship using a normally unconvential
			instrument, the theremin. What we wanted to do was create a
			digital theremin using a LeapMotion for hand detection and tracking,
			mainly for the ease of input and a lack of tools/resources for
			creating an actual theremin (but that's potentially on the to-do
			list), and then input it as MIDI into a pretrained neural network
			using Google Magenta.
		</p>
		<img src = "../techimg/there-2.jpg" class = "project_image">
		<p>
			The user simply waves their hands above the LeapMotion, and after
			they stop playing <span class = "rainbow_element">the neural network
			generates a melody to be played back to them with the tone and style
			of a theremin.</span>
		</p>
	</article>
	<script type = "text/javascript" src = "../js/script.js"></script>
</body>
</html>